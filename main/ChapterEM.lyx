#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass classicthesis
\begin_preamble
\usepackage{algorithm,algpseudocode}
\newref{prop}{name=Proposition~,Name=Proposition~}
\newref{prob}{name=Problem~,Name=Problem~}
\newref{thm}{name=Theorem~,Name=Theorem~}
\newref{chap}{name=Chapter~,Name=Chapter~}
\newref{part}{name=Part~,Name=Part~}
\newref{remark}{name=Remark~,Name=Remark~}
\newref{algo}{name=Algorithm~,Name=Algorithm~}
\newref{lem}{name=Lemma~,Name=Lemma~}
\MakeRobust{\Call}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
theorems-chap
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding default
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Inference on Distributional Data 
\begin_inset CommandInset label
LatexCommand label
name "chap:EM"

\end_inset


\end_layout

\begin_layout Section
Stating the Problem
\end_layout

\begin_layout Standard
In this section, we present a generalization, stated informally in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "chap:Introduction"

\end_inset

, of the problem of inference on discretely observed data from diffusion
 processes.
 
\end_layout

\begin_layout Subsection
The Discretely Observed Data Problem
\end_layout

\begin_layout Standard
First, we review the discretely observed data problem.
 Inference on diffusion processes observed at discrete points in time is
 a topic of interest for many statisticians, in large part due to the prevalence
 of diffusion processes in modeling a wide range of phenomena, and the simultane
ous unavailability of continuous observations from such processes in the
 real world.
 Consider a solution to the one-dimensional stochastic differential equation
\begin_inset Formula 
\begin{equation}
dX_{t}=\mu_{\theta}(X_{t})dt+\sigma_{\theta}(X_{t})dW_{t},\label{eq:sde-with-theta}
\end{equation}

\end_inset

where 
\begin_inset Formula $\theta\in\Theta\subseteq\mathcal{R}^{k}$
\end_inset

 is an unknown parameter vector, 
\begin_inset Formula $W$
\end_inset

 is a Wiener process, and 
\begin_inset Formula $\mu_{\theta},\sigma_{\theta}$
\end_inset

 have known parametric forms up to 
\begin_inset Formula $\theta$
\end_inset

.
 Let 
\begin_inset Formula $\mathcal{X}\subseteq\mathcal{R}$
\end_inset

 be the state space of 
\begin_inset Formula $X$
\end_inset

.
 Suppose that this process is observed to take values 
\begin_inset Formula $\bar{\mathbf{x}}=\{\bar{x}_{t_{1}},\dots,\bar{x}_{t_{n}}\}\in\mathcal{X}^{n}$
\end_inset

 at a collection of times 
\begin_inset Formula $\mathcal{T}=\{t_{1}<\cdots<t_{n}=1\}$
\end_inset

 where 
\begin_inset Formula $t_{1}>0$
\end_inset

.
 Then, the log-likelihood of the data 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 is 
\begin_inset Formula 
\[
\ell(\theta\mid\bar{\mathbf{x}})=\sum_{i=1}^{n}\ell_{i}(\theta),
\]

\end_inset

where 
\begin_inset Formula $\ell_{i}(\theta)=\log p_{\theta,t_{i}-t_{i-1}}(\bar{x}_{t_{i-1}},\bar{x}_{t_{i}})$
\end_inset

 and 
\begin_inset Formula $p_{\theta,t}(x,y)$
\end_inset

 is the transition density associated with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 as usual (we assume 
\begin_inset Formula $x_{t_{0}}=x_{0}$
\end_inset

 is fixed).
 For many specifications of 
\begin_inset Formula $\mu,\sigma$
\end_inset

, 
\begin_inset Formula $p_{t}(x,y;\theta)$
\end_inset

 is not analytically tractable, and thus likelihood-based inference on discretel
y observed data has historically been understood to be quite difficult.
 In particular, maximum likelihood estimation has received significant attention
 in the literature, with proposed approaches ranging from 
\begin_inset CommandInset citation
LatexCommand citet
key "sahalia-2002"

\end_inset

's closed-form analytic likelihood approximations to the simulation-based
 strategies of 
\begin_inset CommandInset citation
LatexCommand citet
key "pedersen-1995"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "durham-gallant-2002"

\end_inset

, and more recently, imputation-based techniques advanced by 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "sorensen-2004"

\end_inset

 offers a comprehensive survey of the range of inferential techniques used
 for the problem of discretely observed diffusions.
\end_layout

\begin_layout Subsection
From Ex-Post to Ex-Ante
\end_layout

\begin_layout Standard
We can re-interpret the discretely observed data problem by supposing we
 lived at 
\begin_inset Formula $t=0$
\end_inset

, and viewing the data 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 not as observed data, but as the values that 
\begin_inset Formula $X$
\end_inset

 would take on with certainty at future times 
\begin_inset Formula $\mathcal{T}$
\end_inset

 (perhaps as prophesized by an omniscient Oracle).
 Modulo philosophical and technical subtleties, the problem has not changed
 materially: we ought to be able to conduct likelihood-based inference using
 
\begin_inset Formula $\ell(\theta\mid\bar{\mathbf{x}}),$
\end_inset

 so long as it is available, as usual.
\end_layout

\begin_layout Standard
This ex-ante perspective allows, however, for a generalization of interest
 to this thesis.
 For we may suppose that instead of knowing the values of 
\begin_inset Formula $X$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

, we know instead with certainty the joint distribution 
\begin_inset Formula $\nu$
\end_inset

 of these values.
 Intuitively, knowing 
\begin_inset Formula $\nu$
\end_inset

 ought to reveal information that we may use to learn about the parameters
 governing the dynamics of 
\begin_inset Formula $X$
\end_inset

; on the other hand, the likelihood function is no longer well-defined,
 and how to proceed with likelihood-based inference is therefore unclear.
\end_layout

\begin_layout Standard
Formally, we consider the family of distributions 
\begin_inset Formula $\mathscr{P}=\{\mathbb{P}_{\theta}:\theta\in\Theta\}$
\end_inset

 induced by solutions to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 with initial condition 
\begin_inset Formula $X_{0}=x_{0}$
\end_inset

 for some fixed 
\begin_inset Formula $x_{0}$
\end_inset

, and let 
\begin_inset Formula $\mathbb{P}_{\theta,X_{\mathcal{T}}}$
\end_inset

 be the law of 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 under 
\begin_inset Formula $\mathbb{P}_{\theta}$
\end_inset

 with density 
\begin_inset Formula $P_{\theta,\mathcal{T}}$
\end_inset

.
 We set 
\begin_inset Formula $\nu$
\end_inset

 as an 
\begin_inset Formula $n$
\end_inset

-dimensional distribution on the usual probability space restricted to times
 
\begin_inset Formula $\mathcal{T}$
\end_inset

, which is absolutely continuous to 
\begin_inset Formula $\mathbb{P}_{\theta,X_{\mathcal{T}}}$
\end_inset

 for all 
\begin_inset Formula $\theta\in\Theta$
\end_inset

.
\end_layout

\begin_layout Standard
We refer to 
\begin_inset Formula $\nu$
\end_inset

 as distributional data, in juxtaposition to the observed data that is given
 in the context of MLE.
 Then, the problem of inference on distributional data is as follows: What,
 if anything can we say about 
\begin_inset Formula $\theta$
\end_inset

, given 
\begin_inset Formula $X_{\mathcal{T}}\sim\nu$
\end_inset

?
\end_layout

\begin_layout Subsection
Characterizing an Estimator
\end_layout

\begin_layout Standard
\begin_inset Marginal
status open

\begin_layout Plain Layout
I think I am getting my 
\begin_inset Quotes eld
\end_inset

laws
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

measures
\begin_inset Quotes erd
\end_inset

, and 
\begin_inset Quotes eld
\end_inset

random variables
\begin_inset Quotes erd
\end_inset

 mixed up throughout this section..
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We first note that the problem of discretely observed data can be heuristically
 understood as a limiting case of the problem of distributional data.
 In particular, we can fix some data 
\begin_inset Formula $\bar{\mathbf{x}}\in\mathcal{X}^{n}$
\end_inset

.
 Then, intuitively, if 
\begin_inset Formula $\nu^{(m)}$
\end_inset

 is a sequence of measures which are absolutely continuous to 
\begin_inset Formula $\mathbb{P}_{\theta,X_{\mathcal{T}}}$
\end_inset

 and converge (in some sense) to the Dirac delta measure 
\begin_inset Formula $\delta_{\bar{\mathbf{x}}}$
\end_inset

, any estimator for 
\begin_inset Formula $\theta$
\end_inset

 given 
\begin_inset Formula $X_{\mathcal{T}}\sim\nu^{(m)}$
\end_inset

 should coincide with the MLE given 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 as 
\begin_inset Formula $m\rightarrow\infty$
\end_inset

.
 One estimator for 
\begin_inset Formula $\theta$
\end_inset

 that seems like it ought to satisfy this criterion is 
\begin_inset Formula 
\begin{equation}
\hat{\theta}=\arg\max_{\theta\in\Theta}\int\log P_{\theta,\mathcal{T}}(\mathbf{x})\nu(d\mathbf{x}).\label{eq:mklde}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We define 
\begin_inset Formula $\Lambda_{\theta}(\nu)\coloneqq\int\log P_{\theta,\mathcal{T}}(\mathbf{x})\nu(d\mathbf{x})$
\end_inset

, and begin with the observation of 
\begin_inset CommandInset citation
LatexCommand citet
key "akaike-1973"

\end_inset

 that maximizing 
\begin_inset Formula $\Lambda_{\theta}(\nu)$
\end_inset

 is equivalent to minimizing the Kullback-Liebler divergence of 
\begin_inset Formula $\mathbb{P}_{\theta,X_{\mathcal{T}}}$
\end_inset

 from 
\begin_inset Formula $\nu$
\end_inset

.
 To see this, simply note that 
\begin_inset Formula 
\begin{eqnarray*}
D(\nu\:||\:\mathbb{P}_{\theta,X_{\mathcal{T}}}) & = & \mathbb{E}_{\nu}\left[\log\frac{d\nu}{d\mathbb{P}_{\theta,X_{\mathcal{T}}}}\right]\\
 & = & \mathbb{E}_{\nu}\left[\log\frac{d\nu}{dx}\right]-\mathbb{E}_{\nu}\left[\log\frac{d\mathbb{P}_{\theta,X_{\mathcal{T}}}}{dx}\right]
\end{eqnarray*}

\end_inset

For this reason, we call the proposed estimator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mklde"

\end_inset

 the minimum Kullback-Liebler divergence estimator (MKLDE).
 We now present a series of semi-technical remarks characterizing the MKLDE
 in a variety of asymptotic settings.
 The first connects the MKLDE to the MLE under model misspecification.
\end_layout

\begin_layout Remark
\begin_inset CommandInset label
LatexCommand label
name "remark:misspecification"

\end_inset

The MLE given discretely observed data at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

 under model misspecification converges in probability to the MKLDE as the
 sample size grows.
 For consider 
\begin_inset Formula $X_{\mathcal{T}}^{(1)},\dots,X_{\mathcal{T}}^{(N)}$
\end_inset

, 
\begin_inset Formula $N$
\end_inset

 i.i.d.
 draws from 
\begin_inset Formula $\nu$
\end_inset

.
 By a result of 
\begin_inset CommandInset citation
LatexCommand citet
before "Theorem 2.2 in"
key "white-1982"

\end_inset

, 
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}\frac{1}{N}\sum_{i=1}^{N}\log P_{\theta,\mathcal{T}}(X_{\mathcal{T}}^{(i)})\xrightarrow{p}\arg\min_{\theta\in\Theta}\mathbb{E}_{\nu}\left[\log\frac{d\nu}{d\mathbb{P}_{\theta,X_{\mathcal{T}}}}\right]
\]

\end_inset

as 
\begin_inset Formula $N\rightarrow\infty$
\end_inset

, and thus by the observation of 
\begin_inset CommandInset citation
LatexCommand citet
key "akaike-1973"

\end_inset

, 
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}\frac{1}{N}\sum_{i=1}^{N}\log P_{\theta,\mathcal{T}}(X_{\mathcal{T}}^{(i)})\xrightarrow{p}\arg\max_{\theta\in\Theta}\Lambda_{\theta}(\nu).
\]

\end_inset


\end_layout

\begin_layout Standard
The intuition for the relationship between the MLE under model misspecification
 and the MKLDE is as follows.
 Recall that a time-homogenous Markov measure is uniquely determined by
 a transition kernel and a marginal measure; an SDE satisfying the requirements
 outlined in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "chap:2"

\end_inset

 along with the initial condition 
\begin_inset Formula $X_{0}=x_{0}$
\end_inset

 therefore admits a bijection between 
\begin_inset Formula $\Theta$
\end_inset

 and 
\begin_inset Formula $\mathscr{P}$
\end_inset

.
 In particular, the elements of 
\begin_inset Formula $\mathscr{P}$
\end_inset

 are the only possible measures that any solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 with initial condition 
\begin_inset Formula $X_{0}=x_{0}$
\end_inset

 could induce.
 If we know 
\begin_inset Formula $X_{\mathcal{T}}\sim\nu$
\end_inset

 (or equivalently, since the empirical law of samples from 
\begin_inset Formula $\nu$
\end_inset

 converges to 
\begin_inset Formula $\nu$
\end_inset

 pointwise almost surely as 
\begin_inset Formula $N\rightarrow\infty$
\end_inset

, if we have infinitely many samples of 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

), and 
\begin_inset Formula $\nu\not=\mathbb{P}_{\theta,X_{\mathcal{T}}}$
\end_inset

 for any 
\begin_inset Formula $\theta$
\end_inset

, it must be the case that we have misspecified our probability model.
 In this case, the best we can do is to find the value of 
\begin_inset Formula $\theta$
\end_inset

 that minimizes the K-L divergence of 
\begin_inset Formula $\mathbb{P}_{\theta}$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

 from 
\begin_inset Formula $\nu$
\end_inset

, which is precisely the MKLDE.
\end_layout

\begin_layout Standard
Having an understanding of the MKLDE as a limit in terms of samples from
 
\begin_inset Formula $\nu$
\end_inset

 is useful because in the real world, few probabilistic forecasts offer
 a functional form for 
\begin_inset Formula $\nu$
\end_inset

.
 More often than not, we will only have access to a finite number of draws
 from 
\begin_inset Formula $\nu$
\end_inset

, and the following remark formalizes the consistency of the MKLDE even
 when perfect access to 
\begin_inset Formula $\nu$
\end_inset

 is not available.
\end_layout

\begin_layout Remark
The MKLDE given an approximation 
\begin_inset Formula $\hat{\nu}$
\end_inset

 to 
\begin_inset Formula $\nu$
\end_inset

 converges in probability to the MKLDE given 
\begin_inset Formula $\nu$
\end_inset

 if 
\begin_inset Formula $\hat{\nu}$
\end_inset

 converges pointwise almost surely to 
\begin_inset Formula $\nu$
\end_inset

.
 In particular, consider the empirical distribution function 
\begin_inset Formula $\hat{\nu}^{(N)}$
\end_inset

 induced by 
\begin_inset Formula $N$
\end_inset

 draws from 
\begin_inset Formula $\nu$
\end_inset

.
 Then, for 
\begin_inset Formula $X_{\mathcal{T}}^{(N,1)},\dots,X_{\mathcal{T}}^{(N,N)}$
\end_inset

 drawn from 
\begin_inset Formula $\hat{\nu}^{(N)}$
\end_inset

, by standard results demonstrating the consistency of plug-in bootstrap
 estimators (see 
\begin_inset CommandInset citation
LatexCommand citet
key "horowitz-2002"

\end_inset

),
\begin_inset Marginal
status open

\begin_layout Plain Layout
Some really ugly notation here.
 Will try to fix.
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\int\log P_{\theta,\mathcal{T}}(\mathbf{x})\hat{\nu}^{(N)}(d\mathbf{x}) & = & \frac{1}{N}\sum_{i=1}^{N}\log P_{\theta,\mathcal{T}}(X_{\mathcal{T}}^{(N,i)})\\
 & \xrightarrow{p} & \Lambda_{\theta}(\nu)
\end{eqnarray*}

\end_inset

and by the 
\begin_inset Formula $\arg\max$
\end_inset

 continuous mapping theorem (see 
\begin_inset CommandInset citation
LatexCommand citet
key "kim-pollard-1990"

\end_inset

), the MKLDE given 
\begin_inset Formula $\hat{\nu}^{(N)}$
\end_inset

 is asymptotically consistent for the MKLDE given 
\begin_inset Formula $\nu$
\end_inset

 as 
\begin_inset Formula $N\rightarrow\infty$
\end_inset

.
\end_layout

\begin_layout Standard
This remark is important in that it says we do not need perfect access to
 
\begin_inset Formula $\nu$
\end_inset

 to carry out inference on 
\begin_inset Formula $\theta$
\end_inset

; rather, and luckily for this thesis, the MKLDE from bootstrapping from
 some approximation to 
\begin_inset Formula $\nu$
\end_inset

 diverges from the MKLDE given 
\begin_inset Formula $\nu$
\end_inset

 with probability zero asymptotically.
 In other words, though we can only ever draw finitely many samples from
 
\begin_inset Formula $\nu$
\end_inset

 in the real world, the simulation-based inference techniques to be presented
 below can offer high quality estimates of 
\begin_inset Formula $\theta$
\end_inset

 for a sufficiently large sample size.
\begin_inset Marginal
status open

\begin_layout Plain Layout
Some of these remarks are a little sketchy, but I don't think have the technical
 background to make them more precise.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The final remark we offer formalizes the intuition that began this subsection.
\end_layout

\begin_layout Remark
Consider a sequence of random variables 
\begin_inset Formula $X_{\mathcal{T}}^{(m)}$
\end_inset

 with law 
\begin_inset Formula $\nu^{(m)}$
\end_inset

 absolutely continuous to 
\begin_inset Formula $\mathbb{P}_{\theta,X_{\mathcal{T}}}$
\end_inset

, which converge in law to a constant 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

.
 Then, the sequence of MKLDEs given 
\begin_inset Formula $X_{\mathcal{T}}^{(m)}$
\end_inset

 converges in probability to the MLE given 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

.
 For fix any 
\begin_inset Formula $N,$
\end_inset

 and consider 
\begin_inset Formula $N$
\end_inset

 i.i.d.
 draws 
\begin_inset Formula $X_{\mathcal{T}}^{(m,1)},\dots,X_{\mathcal{T}}^{(m,N)}$
\end_inset

 from 
\begin_inset Formula $\nu^{(m)}$
\end_inset

.
 Then, by the continuous mapping theorem, 
\begin_inset Formula 
\[
\frac{1}{N}\sum_{i=1}^{N}\log P_{\theta,\mathcal{T}}(X_{\mathcal{T}}^{(m,i)})\xrightarrow{p}\log P_{\theta,\mathcal{T}}(\bar{\mathbf{x}}).
\]

\end_inset

An application of the 
\begin_inset Formula $\arg\max$
\end_inset

 continuous mapping theorem, along with suitable regularity conditions on
 
\begin_inset Formula $\log P_{\theta,\mathcal{T}}$
\end_inset

, gives the desired result.
 In particular, for solutions to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 with state space 
\begin_inset Formula $\mathcal{X}=\mathcal{R}$
\end_inset

, it can be easily verified that a sequence of Gaussian random variables
 with mean 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 and marginal variance 
\begin_inset Formula $1/m$
\end_inset

 reduces the MKLDE to the MLE in the limit.
\end_layout

\begin_layout Standard
This remark formalizes the intuition that an Oracle prophesizing with certainty
 
\begin_inset Formula $X$
\end_inset

 taking on values 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 at 
\begin_inset Formula $\mathcal{T}$
\end_inset

 is the same as already having observed the values, once again bracketing
 some philosophical and technical details.
 So, as an Oracle becomes increasingly sure that 
\begin_inset Formula $X$
\end_inset

 will take on values 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

, the probability that the outcomes of inference on distributional data
 and on discretely observed data diverge should become increasingly small.
 
\end_layout

\begin_layout Standard
These remarks together give an account of why the estimator defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mklde"

\end_inset

 is a statistically meaningful estimator for 
\begin_inset Formula $\theta$
\end_inset

 in the context of distributional data.
 The MKLDE minimizes the K-L divergence of the law of 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 under 
\begin_inset Formula $\mathbb{P}_{\theta}$
\end_inset

 from 
\begin_inset Formula $\nu$
\end_inset

 can be understood as performing MLE under model misspecification with infinite
 data (furthermore, we may bootstrap from a sufficiently good approximation
 to 
\begin_inset Formula $\nu$
\end_inset

 if we don't have perfect access to 
\begin_inset Formula $\nu$
\end_inset

).
 This interpretation is intuitively pleasing, and together with the observation
 that the MKLDE reduces to the MLE in the limit for a sequence of laws of
 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 tending to a degenerate measure, the proposed estimator seems to be a reasonabl
e one.
 
\end_layout

\begin_layout Standard
However, given the unavailability of a transition density for most diffusion
 processes, it quickly becomes clear that 
\begin_inset Formula $\Lambda_{\nu}(\theta)$
\end_inset

 has no obvious functional form for all but the most specific choices of
 
\begin_inset Formula $\mu_{\theta},\sigma_{\theta}$
\end_inset

, and 
\begin_inset Formula $\nu$
\end_inset

.
 Therefore, solving for the MKLDE is impossible from an analytic standpoint;
 as such, the next section will develop a simulation-based strategy for
 finding the MKLDE.
\end_layout

\begin_layout Section
Simulation-Based K-L Divergence Minimization
\end_layout

\begin_layout Standard
A particular strand of literature attempting to address the issue of likelihood-
based inference for discretely observed diffusions has exploited simulation-base
d inference techniques.
 This approach began with the seminal paper of 
\begin_inset CommandInset citation
LatexCommand citet
key "pedersen-1995"

\end_inset

.
 The key issue is that were the continuous path of the data available, the
 likelihood function would be given by Girsanov's formula; however, with
 only discrete observations, the likelihood function is unavailable.
 This framing of the problem suggests that it can be considered a problem
 of missing data, and 
\begin_inset CommandInset citation
LatexCommand citet
key "roberts-stramer-2001,elerian-chib-shephard-2001"

\end_inset

, and others simultaneously realized that simulation of diffusion bridges
 could be leveraged to supply such data.
 A method of particular interest is that of 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

, where a Monte Carlo expectation-maximization (MCEM) algorithm following
 
\begin_inset CommandInset citation
LatexCommand citet
key "dempster-1977"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "wei-tanner-1990"

\end_inset

 is proposed, using the Exact Algorithm for the simulation of diffusion
 bridges developed in 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-roberts-2005"

\end_inset

.
\end_layout

\begin_layout Standard
We generalize the results of 
\begin_inset CommandInset citation
LatexCommand citet
key "roberts-stramer-2001,beskos-2006,bladt-sorensen-2014"

\end_inset

 to propose a simulation-based method to find the MKLDE.
 As noted above, even writing out 
\begin_inset Formula $\Lambda_{\nu}(\theta)$
\end_inset

, let alone maximizing it analytically, is impossible in general.
 However, noting the close parallels between MLE on discretely observed
 data and MKLDE on distributional data, we develop in this section a Monte
 Carlo simulation strategy which converges to the MKLDE for a sufficiently
 large number of iterations.
\end_layout

\begin_layout Standard
\begin_inset Marginal
status open

\begin_layout Plain Layout
I think there is some identifiability condition that needs to hold.
 For instance, if 
\begin_inset Formula $\nu$
\end_inset

 is a one dimensional distribution, MKLDE will not converge in the case
 of an OU process since the parameters are not identified.
 Not sure if it's just the case that 
\begin_inset Formula $n\geq2$
\end_inset

 or if it's something more specific.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The Case of Unit Diffusion Coefficient
\end_layout

\begin_layout Standard
We will first consider the highly specific case of 
\begin_inset Formula $\sigma(x;\theta)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 being equal to unity so that we may focus deriving the general shape of
 what a simulation-strategy should look like.
 In particular, in this subsection, we will derive an EM algorithm that
 will maximize 
\begin_inset Formula $\Lambda_{\theta}(\nu)$
\end_inset

, in a manner analagous to the EM algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 which maximizes the log-likelihood of discretely observed data.
 We first prove the central result of this chapter, which will allow us
 to derive such an algorithm, recalling that the likelihood of a continuous
 path of a solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 where 
\begin_inset Formula $\sigma(x;\theta)=1$
\end_inset

 is readily available by Girsanov's theorem.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:em"

\end_inset

Fix an parameter estimate 
\begin_inset Formula $\tilde{\theta}\in\Theta$
\end_inset

 and law 
\begin_inset Formula $\nu$
\end_inset

 satisfying the usual requirements.
 Then, 
\begin_inset Formula 
\begin{equation}
\arg\max_{\theta\in\Theta}\Lambda_{\theta}(\nu)-\Lambda_{\tilde{\theta}}(\nu)=\arg\max_{\theta\in\Theta}\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[\log P_{\theta}(X)\right],\label{eq:em-equation}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbb{P}_{\tilde{\theta}}^{\nu}$
\end_inset

 is the Baudoin 
\begin_inset Formula $(X_{\mathcal{T}},\nu)$
\end_inset

-conditioning of the measure induced by a solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 governed by parameter 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 with constant initial condition, and 
\begin_inset Formula $X$
\end_inset

 is a random path distributed according to 
\begin_inset Formula $\mathbb{P}_{\tilde{\theta}}^{\nu}$
\end_inset

.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $X$
\end_inset

 be a solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 with constant initial condition as usual.
 We begin by re-expressing the objective function on the left hand side
 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:em-equation"

\end_inset

 as 
\begin_inset Formula 
\begin{eqnarray}
\Lambda_{\theta}(\nu)-\Lambda_{\tilde{\theta}}(\nu) & = & \int\log P_{\theta}(\mathbf{x})\nu(d\mathbf{x})-\Lambda_{\tilde{\theta}}(\nu)\nonumber \\
 & = & \int\log\int P_{\theta}(\mathbf{x},z)dz\nu(d\mathbf{x})-\Lambda_{\tilde{\theta}}(\nu),\label{eq:firststepem}
\end{eqnarray}

\end_inset

where we take
\series bold
 
\begin_inset Formula $Z$
\end_inset

 
\series default
to be missing data representing the path of 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $t\in[0,1]$
\end_inset

.
 Then, following standard arguments in the derivation of EM algorithms (see
 
\begin_inset CommandInset citation
LatexCommand citet
key "borman-2006"

\end_inset

), we can find from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:firststepem"

\end_inset

 that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align}
\Lambda_{\theta}(\nu)-\Lambda_{\tilde{\theta}}(\nu) & \geq\iint\log\left(\frac{P_{\theta}(\mathbf{x},z)}{P_{\tilde{\theta}}(z\mid\mathbf{x})}\right)P_{\tilde{\theta}}(z\mid\mathbf{x})dz\nu(d\mathbf{x})-\Lambda_{\tilde{\theta}}(\nu)\nonumber \\
 & =\iint\log\left(\frac{P_{\theta}(\mathbf{x},z)}{P_{\tilde{\theta}}(z\mid\mathbf{x})P_{\tilde{\theta}}(\mathbf{x})}\right)\mathbb{P}_{\tilde{\theta}}(dz\mid\mathbf{x})\nu(d\mathbf{x}).\label{eq:Deltatn}
\end{align}

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Marginal
status open

\begin_layout Plain Layout
This step is wrong.
 But the simulations work and it seems so intuitive.
 What am I missing?
\end_layout

\end_inset

Assuming suitable regularity conditions, note that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Deltatn"

\end_inset

 can be re-written as 
\begin_inset Formula 
\[
\Delta(\theta\mid\tilde{\theta})\coloneqq\int\log\left(\frac{P_{\theta}(\mathbf{x},z)}{P_{\tilde{\theta}}(z\mid\mathbf{x})P_{\tilde{\theta}}(\mathbf{x})}\right)\mathbb{P}_{\tilde{\theta}}^{\nu}(dz)
\]

\end_inset

by the definition of a Baudoin conditioning.
 Now, let 
\begin_inset Formula $Q(\theta\mid\tilde{\theta})\coloneqq\Lambda^{\nu}(\tilde{\theta})+\Delta(\theta\mid\tilde{\theta})\leq\Lambda^{\nu}(\theta)$
\end_inset

.
 Once again, by standard arguments, any 
\begin_inset Formula $\theta$
\end_inset

 that increases 
\begin_inset Formula $Q(\theta\mid\tilde{\theta})$
\end_inset

 must also increase 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

.
 To maximize this increase, it suffices to solve for
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}Q(\theta\mid\tilde{\theta})=\arg\max_{\theta\in\Theta}\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[\log P_{\theta}(X)\right],
\]

\end_inset

and the theorem follows as desired.
\end_layout

\begin_layout Standard
Note that when 
\begin_inset Formula $\nu=\delta_{\bar{\mathbf{x}}}$
\end_inset

 (in some limiting as in the remarks earlier) for fixed data 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:em-equation"

\end_inset

 reduces to 
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}\ell(\theta\mid\bar{\mathbf{x}})-\ell(\tilde{\theta}\mid\bar{\mathbf{x}})=\arg\max_{\theta}\mathbb{E}_{Z\mid\bar{\mathbf{x}},\tilde{\theta}}\left[\log P_{\theta}(Z,\bar{\mathbf{x}})\right]
\]

\end_inset

where the expectation is taken over diffusion bridges 
\begin_inset Formula $Z$
\end_inset

, as presented in 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 for the case of discretely observed data.
 
\end_layout

\begin_layout Standard
By Girsanov's theorem, we know that the complete-data log-likelihood is
 
\begin_inset Formula 
\[
\log P_{\theta}(X)=\int_{0}^{1}\mu_{\theta}(X_{s})dx_{s}-\frac{1}{2}\int_{0}^{1}\mu_{\theta}^{2}(X_{s})ds,
\]

\end_inset

and thus the expectation step of an EM algorithm would consist of computing
\begin_inset Formula 
\begin{equation}
Q(\theta\mid\tilde{\theta})=\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[\int_{0}^{1}\mu_{\theta}(X_{s})dX_{s}-\frac{1}{2}\int_{0}^{1}\mu_{\theta}^{2}(X_{s})ds\right].\label{eq:simple-q}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Though 
\begin_inset Formula $Q$
\end_inset

 is still intractable, it is suggestive a Monte Carlo implementation of
 the EM algorithm following 
\begin_inset CommandInset citation
LatexCommand citet
key "wei-tanner-1990"

\end_inset

, if we could sample from 
\begin_inset Formula $\mathbb{P}_{\tilde{\theta}}^{\nu}$
\end_inset

.
 But before developing such an implementation, we generalize 
\begin_inset Formula $Q$
\end_inset

 for the case of diffusion coefficient known only up to 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Subsection
Augmentation Under the Lamperti Transform
\end_layout

\begin_layout Standard
The restriction of the diffusion coefficient to unity makes the 
\begin_inset Formula $Q$
\end_inset

 function presented above almost useless for practical purposes.
 In this section, we relax this assumption and derive an EM algorithm suitable
 for general use.
\end_layout

\begin_layout Standard
Recall that for any solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

, we may define the Lamperti transform 
\begin_inset Formula $\eta_{\theta}(x)$
\end_inset

, where 
\begin_inset Formula 
\begin{equation}
\eta_{\theta}(x)=\int_{x^{*}}^{x}\frac{1}{\sigma_{\theta}(y)}dy,\label{eq:lamperti}
\end{equation}

\end_inset

for appropriate but otherwise arbitrary 
\begin_inset Formula $x^{*}$
\end_inset

.
 If we set 
\begin_inset Formula $Y_{t}(\theta)=\eta_{\theta}(X_{t})$
\end_inset

, by Ito's lemma, 
\begin_inset Formula $Y$
\end_inset

 is the solution to the stochastic differential equation
\begin_inset Formula 
\begin{equation}
dY_{t}=\alpha_{\theta}(Y_{t})dt+dW_{t},\label{eq:eta-y}
\end{equation}

\end_inset

where 
\begin_inset Formula 
\[
\alpha_{\theta}(y)=\frac{\mu_{\theta}(\eta_{\theta}^{-1}(y))}{\sigma_{\theta}(\eta_{\theta}^{-1}(y))}-\frac{1}{2}\sigma_{\theta}^{'}(\eta_{\theta}^{-1}(y)).
\]

\end_inset

The Lamperti transform is therefore a continuous transformation that reduces
 the diffusion coefficient of any SDE to unity; in conjunction with the
 change of variables formula, it seems like a trivial task to adapt 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:simple-q"

\end_inset

 for general diffusions.
\end_layout

\begin_layout Standard
\begin_inset Marginal
status open

\begin_layout Plain Layout
I've read and re-read Bladt/Beskos many times and I just can't really get
 my hands on what we're really doing in a formal sense here..
\end_layout

\end_inset

However, 
\begin_inset CommandInset citation
LatexCommand citet
key "bladt-sorensen-2014"

\end_inset

 point out that the issue with the Lamperti transform is that the distribution
 of 
\begin_inset Formula $Y$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

 is now dependent on the parameter 
\begin_inset Formula $\theta$
\end_inset

, while the EM algorithm requires that the given data remain fixed.
 We adapt their proposed solution to our problem.
 For each 
\begin_inset Formula $i=1,\dots,n$
\end_inset

 we consider 
\begin_inset Formula $t_{i-1}\leq t\leq t_{i}$
\end_inset

.
 Let 
\begin_inset Formula $X$
\end_inset

 be a path on 
\begin_inset Formula $\mathbb{P}_{\tilde{\theta}}^{\nu}$
\end_inset

 and let 
\begin_inset Formula $Z(\tilde{\theta})=\eta_{\tilde{\theta}}(X)$
\end_inset

.
 Setting 
\begin_inset Formula $H_{\tilde{\theta}\rightarrow\theta}=\eta_{\theta}\circ\eta_{\tilde{\theta}}^{-1}$
\end_inset

, we define 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{*}(\theta,\tilde{\theta}) & = & Z_{t}(\tilde{\theta})+\frac{(t_{i}-t)(H_{\tilde{\theta}\rightarrow\theta}(Z_{t_{i-1}}(\tilde{\theta}))-Z_{t_{i-1}}(\tilde{\theta}))}{t_{i}-t_{i-1}}+\\
 &  & \frac{(t-t_{i-1})(H_{\tilde{\theta}\rightarrow\theta}(Z_{t_{i}}(\tilde{\theta}))-Z_{t_{i}}(\tilde{\theta}))}{t_{i}-t_{i-1}}.
\end{eqnarray*}

\end_inset

Note that 
\begin_inset Formula $Y_{t_{i-1}}^{*}(\theta,\tilde{\theta})=H_{\tilde{\theta}\rightarrow\theta}(Z_{t_{i-1}}(\tilde{\theta}))$
\end_inset

 and 
\begin_inset Formula $Y_{t_{i}}^{*}(\theta,\tilde{\theta})=H_{\tilde{\theta}\rightarrow\theta}(Z_{t_{i}}(\tilde{\theta}))$
\end_inset

.
 Then, letting 
\begin_inset Formula $A_{\theta}(u)=\int_{u^{*}}^{u}\alpha_{\theta}$
\end_inset

 for suitable but otherwise arbitrary 
\begin_inset Formula $u^{*}$
\end_inset

, we follow 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "papa-roberts-2012"

\end_inset

 to find the conditional expectation of the relevant continuous log-likelihood
 function of 
\begin_inset Formula $Y_{t}^{*}(\theta,\tilde{\theta})$
\end_inset

, 
\begin_inset Formula 
\begin{eqnarray}
Q(\theta\mid\tilde{\theta}) & = & \mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[A_{\theta}(Z_{t_{n}}(\theta))\right]-A_{\theta}(\eta_{\theta}(x_{0}))-\nonumber \\
 &  & \sum_{i=1}^{n}\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[\log(\sigma_{\theta}(X_{t_{i}}))\right]-\nonumber \\
 &  & \frac{1}{2}\sum_{i=1}^{n}\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[(Z_{t_{i}}(\theta)-Z_{t_{i-1}}(\theta))^{2}/(t_{i}-t_{i-1})\right]-\nonumber \\
 &  & \frac{1}{2}\sum_{i=1}^{n}\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu}}\left[\int_{t_{i-1}}^{t_{i}}\left(\alpha_{\theta}^{2}+\alpha_{\theta}^{'}\right)\left(Y_{t}^{*}(\theta,\tilde{\theta})\right)dt\right],\label{eq:final-q}
\end{eqnarray}

\end_inset

recalling that 
\begin_inset Formula $x_{t_{0}}=x_{0}$
\end_inset

 is fixed.
 By writing the expectation with respect to 
\begin_inset Formula $\mathbb{P}_{\tilde{\theta}}^{\nu}$
\end_inset

, we keep the distribution at 
\begin_inset Formula $\mathcal{T}$
\end_inset

 of the reference process 
\begin_inset Formula $X$
\end_inset

 under a Lamperti transformation fixed, which allows the EM algorithm to
 converge properly.
 We note that we can equivalently write 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
Q(\theta\mid\tilde{\theta}) & = & \mathbb{E}_{\nu_{n}}\left[A_{\theta}(\eta_{\theta}(X_{t_{n}}))\right]-A_{\theta}(\eta_{\theta}(x_{0}))-\nonumber \\
 &  & \sum_{i=1}^{n}\mathbb{E}_{\nu_{i}}\left[\log(\sigma_{\theta}(X_{t_{i}}))\right]-\nonumber \\
 &  & \frac{1}{2}\sum_{i=1}^{n}\mathbb{E}_{\nu_{i-1,i}}\left[(\eta_{\theta}(X_{t_{i}})-\eta_{\theta}(X_{t_{i-1}}))^{2}/(t_{i}-t_{i-1})\right]-\nonumber \\
 &  & \frac{1}{2}\sum_{i=1}^{n}(t_{i}-t_{i-1})\mathbb{E}_{\mathbb{P}_{\tilde{\theta}}^{\nu},U_{i}}\left[\left(\alpha_{\theta}^{2}+\alpha_{\theta}^{'}\right)\left(Y_{U_{i}}^{*}(\theta,\tilde{\theta})\right)\right].\label{eq:final-q-1}
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $\nu_{i}$
\end_inset

 is the marginal distribution of the 
\begin_inset Formula $i$
\end_inset

th coordinate of 
\begin_inset Formula $\nu$
\end_inset

, 
\begin_inset Formula $\nu_{i,j}$
\end_inset

 is the joint distribution of the 
\begin_inset Formula $(i,j)$
\end_inset

-th coordinates of 
\begin_inset Formula $\nu$
\end_inset

, and 
\begin_inset Formula $U_{i}$
\end_inset

 is an independent uniform random variable on 
\begin_inset Formula $[t_{i-1},t_{i}]$
\end_inset

.
 This follows by the fact that the expectations in all terms but last in
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:final-q"

\end_inset

 are taken only over the values of 
\begin_inset Formula $X$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

, and by standard Monte Carlo integration results.
 
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MCEM for the MKLDE Given Distributional Data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
Function{$Q$}{$
\backslash
theta, 
\backslash
tilde{
\backslash
theta}, 
\backslash
nu, N$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$Q 
\backslash
gets $
\backslash
Call{mean}{$A_{
\backslash
theta}(
\backslash
eta_{
\backslash
theta}($
\backslash
Call{sample}{$
\backslash
nu, N$}$[,n]))$}}
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$Q 
\backslash
gets Q 
\backslash
:-
\backslash
: $
\backslash
Call{mean}{$A_{
\backslash
theta}(
\backslash
eta_{
\backslash
theta}($
\backslash
Call{sample}{$
\backslash
nu, N$}$[,1]))$}}
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$Q 
\backslash
gets Q 
\backslash
:-
\backslash
: $
\backslash
Call{sum}{
\backslash
Call{column-means}{$
\backslash
log(
\backslash
sigma_{
\backslash
theta}($
\backslash
Call{sample}{$
\backslash
nu, N$}$)$}}}
\end_layout

\begin_layout Plain Layout

	
\backslash
State{TO BE COMPLETED}
\end_layout

\begin_layout Plain Layout

	
\backslash
For{$i = 1,
\backslash
dots, N$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State{$Y_
\backslash
mathcal{T}^{(i)} 
\backslash
gets 
\backslash
eta_{
\backslash
theta}(X_
\backslash
mathcal{T}^{(i)})$, where $X_
\backslash
mathcal{T}^{(i)} 
\backslash
sim 
\backslash
nu$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State{$Y^{*,(i)} 
\backslash
gets$ 
\backslash
Call{exact $
\backslash
nu$-bridge}{$
\backslash
mu_{
\backslash
tilde{
\backslash
theta}}, 
\backslash
sigma_{
\backslash
tilde{
\backslash
theta}}, 
\backslash
nu 
\backslash
circ 
\backslash
eta_
\backslash
theta^{-1}, 
\backslash
mathbf{T}$}}
\end_layout

\begin_layout Plain Layout

		
\backslash
State{$U^{(i)} 
\backslash
gets$ 
\backslash
Call{sample}{$
\backslash
mathbf{T}$}}
\end_layout

\begin_layout Plain Layout

	
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return{${
\backslash
displaystyle
\backslash
frac{1}{N} 
\backslash
sum^{N}_{i=1}  [
\backslash
alpha_{
\backslash
tilde{
\backslash
theta}}(Y_1^{(i)}) - 
\backslash
alpha_{
\backslash
tilde{
\backslash
theta}}(Y_0^{(i)})] - 
\end_layout

\begin_layout Plain Layout

					 			 
\backslash
frac{1}{2N}
\backslash
sum^{N}_{i=1} (
\backslash
mu_{
\backslash
tilde{
\backslash
theta}}^2 + 
\backslash
mu_{
\backslash
tilde{
\backslash
theta}}^{'})(Y^{*,(i)}_{U^{(i)}})}$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{find-mklde}{$
\backslash
theta_0, 
\backslash
nu, N$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$
\backslash
tilde{
\backslash
theta} 
\backslash
gets 
\backslash
theta_0$}	
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$
\backslash
mathbf{repeat
\backslash
;} 
\backslash
tilde{
\backslash
theta} = 
\backslash
arg 
\backslash
max_{
\backslash
theta 
\backslash
in 
\backslash
Theta} Q(
\backslash
theta, 
\backslash
tilde{
\backslash
theta}, 
\backslash
nu, N)
\backslash
;$$
\backslash
mathbf{until}$ convergence}
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return{$
\backslash
tilde{
\backslash
theta}$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "algo:mcem"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
While 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:final-q-1"

\end_inset

 remains hopelessly intractable, it immediately suggests a Monte Carlo EM
 (MCEM) scheme, assuming we have a way of sampling from the Baudoin bridge
 measure 
\begin_inset Formula $\mathbb{P}_{\theta}^{\nu}$
\end_inset

.
 Suppose for the moment we do: the function 
\noun on
exact baudoin-bridge
\noun default
(
\series bold
\noun on

\begin_inset Formula $\theta,\nu,M,\Delta$
\end_inset


\series default
\noun default
) will return discrete observations at 
\begin_inset Formula $\{0,\delta,\dots,M\delta\}$
\end_inset

 for 
\begin_inset Formula $\delta=\Delta/M$
\end_inset

 of an exact diffusion on the Baudoin 
\begin_inset Formula $(X_{0,\Delta},\nu)$
\end_inset

-conditioning of 
\begin_inset Formula $\mathbb{P}_{[0,\Delta]}$
\end_inset

.
 We also suppose we have a function 
\noun on
sample(
\begin_inset Formula $\nu,N$
\end_inset

)
\noun default
 which returns 
\begin_inset Formula $N$
\end_inset

 samples from 
\begin_inset Formula $\nu$
\end_inset

 in a 
\begin_inset Formula $N\times n$
\end_inset

 matrix.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "algo:mcem"

\end_inset

 presents an MCEM algorithm which, given 
\begin_inset Formula $\nu$
\end_inset

, converges to the MKLDE as described in 
\begin_inset CommandInset citation
LatexCommand citet
key "wei-tanner-1990"

\end_inset

.
 We note that there is no need to simulate an entire bridge over 
\begin_inset Formula $[0,1]$
\end_inset

 to compute the final term of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:final-q-1"

\end_inset

: due to the Markov property, for any realization of 
\begin_inset Formula $U_{i}$
\end_inset

 , we merely need to take the value at 
\begin_inset Formula $U\mod t_{i-1}$
\end_inset

 of a Baudoin 
\begin_inset Formula $(X_{0,t_{i}-t_{i-1}},\nu_{i})$
\end_inset

-bridge simulated on 
\begin_inset Formula $[0,t_{i}-t_{i-1}]$
\end_inset

.
 We simulate new samples from 
\begin_inset Formula $\nu$
\end_inset

 at every possible point in the algorithm as a simple method of Monte Carlo
 variance reduction.
\end_layout

\begin_layout Standard
We have thus partially answered the question of inference on distributional
 data.
 There is in fact a meaningful estimate of 
\begin_inset Formula $\theta$
\end_inset

 when we are given that 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 is distributed according to 
\begin_inset Formula $\nu$
\end_inset

: this is precisely the value of 
\begin_inset Formula $\theta$
\end_inset

 which minimizes the K-L divergence of 
\begin_inset Formula $\mathbb{P}_{\theta,\mathcal{T}}$
\end_inset

 from 
\begin_inset Formula $\nu$
\end_inset

, and we can find such a value using 
\begin_inset CommandInset ref
LatexCommand formatted
reference "algo:mcem"

\end_inset

.
 However, to implement this MCEM scheme, we must have a way to sample from
 the Baudoin 
\begin_inset Formula $(X_{0,\Delta},\nu)$
\end_inset

-conditioning of 
\begin_inset Formula $\mathbb{P}_{[0,\Delta]}$
\end_inset

; in other words, given 
\begin_inset Formula $\theta$
\end_inset

, we must be able to draw from the imputed distribution of 
\begin_inset Formula $X$
\end_inset

 conditional on 
\begin_inset Formula $X_{0,\Delta}\sim\nu$
\end_inset

.
 We turn to this question in the next chapter.
\end_layout

\end_body
\end_document
