#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass classicthesis
\begin_preamble
\usepackage{algorithm,algpseudocode}
\newref{prop}{name=Proposition~,Name=Proposition~}
\newref{prob}{name=Problem~,Name=Problem~}
\newref{thm}{name=Theorem~,Name=Theorem~}
\newref{chap}{name=Chapter~,Name=Chapter~}
\newref{part}{name=Part~,Name=Part~}
\newref{algo}{name=Algorithm~,Name=Algorithm~}
\newref{lem}{name=Lemma~,Name=Lemma~}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
theorems-chap
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding default
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Inference on Distributional Data 
\begin_inset CommandInset label
LatexCommand label
name "chap:EM"

\end_inset


\end_layout

\begin_layout Section
An Oracle Under the Weather
\end_layout

\begin_layout Subsection
Motivation and Formalizing the Problem
\end_layout

\begin_layout Standard
As discussed in the Introduction, inference on discretely observed data
 from diffusion processes, and stochastic processes more generally, has
 been a topic of interest for many years.
 The canonical example of such an inference problem is maximum likelihood
 estimation.
 For example, consider a solution to the one-dimensional stochastic differential
 equation
\begin_inset Formula 
\begin{equation}
dX_{t}=\mu_{\theta}(X_{t})dt+\sigma_{\theta}(X_{t})dW_{t},\label{eq:sde-with-theta}
\end{equation}

\end_inset

where 
\begin_inset Formula $\theta\in\Theta\subseteq\mathcal{R}^{k}$
\end_inset

 is an unknown parameter vector, 
\begin_inset Formula $W$
\end_inset

 is a standard Wiener process, and 
\begin_inset Formula $\mu_{\theta},\sigma_{\theta}$
\end_inset

 have known parametric forms up to 
\begin_inset Formula $\theta$
\end_inset

.
 Suppose that this process is observed at a collection of times 
\begin_inset Formula $\mathcal{T}=\{0=t_{0}<t_{1}<\cdots<t_{n}=1\}$
\end_inset

.
 Then, the average log-likelihood of the data 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 is 
\begin_inset Formula 
\[
\ell(\theta\mid X_{\mathcal{T}})=\frac{1}{n}\sum_{i=1}^{n}\ell_{i}(\theta),
\]

\end_inset

where 
\begin_inset Formula $\ell_{i}(\theta)=\log p_{t_{i}-t_{i-1}}(X_{t_{i-1}},X_{t};\theta)$
\end_inset

 and 
\begin_inset Formula $p_{t}(x,y;\theta)$
\end_inset

 is the transition density associated with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 as usual.
 For many specifications of 
\begin_inset Formula $\mu,\sigma$
\end_inset

, 
\begin_inset Formula $p_{t}(x,y;\theta)$
\end_inset

 is not analytically tractable, and thus likelihood-based inference on discretel
y observed data has historically been understood to be quite difficult.
\end_layout

\begin_layout Standard
One strand of literature attempting to address the issue of likelihood-based
 inference for discretely observed diffusions has exploited simulation-based
 inference techniques.
 This approach began with the seminal paper of 
\begin_inset CommandInset citation
LatexCommand citet
key "pedersen-1995"

\end_inset

.
 The key issue is that were the continuous path of the data available, the
 likelihood function would be given by Girsanov's formula; however, with
 only discrete observations, the likelihood function is unavailable.
 This framing of the problem suggests that it can be considered a problem
 of missing data, and 
\begin_inset CommandInset citation
LatexCommand citet
key "roberts-stramer-2001,elerian-chib-shephard-2001"

\end_inset

, and others simultaneously realized that simulation of diffusion bridges
 could be leveraged to supply such data.
 A method of particular interest is that of 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

, where a Monte Carlo expectation-maximization (MCEM) algorithm following
 
\begin_inset CommandInset citation
LatexCommand citet
key "dempster-1977"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "wei-tanner-1990"

\end_inset

 is proposed, using the Exact Algorithm for the simulation of diffusion
 bridges developed in 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-roberts-2005"

\end_inset

.
\end_layout

\begin_layout Standard
In this chapter, we will consider a generalized version of the problem of
 discrete observations, stated informally below, and attempt to expand the
 reach of the referenced simulation-based inference techniques to conduct
 inference in this new setting.
\end_layout

\begin_layout Problem
\begin_inset CommandInset label
LatexCommand label
name "prob:problem-oracle"

\end_inset

Consider the solution 
\begin_inset Formula $X$
\end_inset

 to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

.
 We live at time 
\begin_inset Formula $t=-\epsilon$
\end_inset

, with an omniscient Oracle as our neighbour.
 We are interested in carrying out maximum likelihood estimation on 
\begin_inset Formula $\theta$
\end_inset

, and normally rely on the Oracle to tell us precisely what values 
\begin_inset Formula $X$
\end_inset

 will take on at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

; in particular, the EM algorithm developed by our other neighbours 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 offers a maximum likelihood estimate given the Oracle's soothsayings.
\end_layout

\begin_layout Problem
However, recently the Oracle has been feeling a little under the weather,
 and her vision into the future has become clouded.
 Instead of being able to tell us the exact values of 
\begin_inset Formula $X$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

, she can only offer us a probability distribution 
\begin_inset Formula $\nu$
\end_inset

 over the values of 
\begin_inset Formula $X$
\end_inset

 at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

.
 Conducting inference on 
\begin_inset Formula $\theta$
\end_inset

 waits for no one, not even a sick Oracle, but without even a well-defined
 likelihood function, whatever will we do?
\end_layout

\begin_layout Standard
We formalize 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prob:problem-oracle"

\end_inset

 as follows.
 Let 
\begin_inset Formula $\mathscr{P}=\{\mathbb{P}(\cdot\mid\theta)\}_{\theta\in\Theta}$
\end_inset

 be a family of distributions induced by stationary solutions to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

, where each 
\begin_inset Formula $\mathbb{P}(\cdot\mid\theta)\in\mathscr{P}$
\end_inset

 has Lebesgue density 
\begin_inset Formula $p_{\theta}$
\end_inset

 and satisfies suitable regularity assumptions.
 Let 
\begin_inset Formula $\nu$
\end_inset

 be a known distribution law on the usual probability space restricted to
 times 
\begin_inset Formula $\mathcal{T}$
\end_inset

, and assume 
\begin_inset Formula $\nu$
\end_inset

 has a Lebesgue density 
\begin_inset Formula $v$
\end_inset

.
 Furthermore, assume 
\begin_inset Formula $\nu$
\end_inset

 is absolutely continuous with respect to any member of 
\begin_inset Formula $\mathscr{P}$
\end_inset

.
 We will refer to 
\begin_inset Formula $\nu$
\end_inset

 as distributional data, in contrast to the observed data that is given
 in the context of MLE.
 Though we would like to conduct some analog of maximum likelihood estimation
 given 
\begin_inset Formula $\nu$
\end_inset

, it is not at all clear what quantity as a function of 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\nu$
\end_inset

 we ought to maximize.
 Furthermore, it is not even clear given such a quantity 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

, what exactly its relation to 
\begin_inset Formula $\mathbb{P}_{\mathcal{T}}(\cdot\mid\theta)$
\end_inset

 or the properties of 
\begin_inset Formula $\arg\max\Lambda^{\nu}(\theta)$
\end_inset

 would be.
\end_layout

\begin_layout Standard
At this point, the single heuristic available to us is that for fixed data
 
\begin_inset Formula $\bar{\mathbf{x}}=\{\bar{x}_{t_{0}},\dots,\bar{x}_{t_{n}}\}$
\end_inset

, it ought to be the case that 
\begin_inset Formula $\arg\max\Lambda^{\delta_{\bar{\mathbf{x}}}}(\theta)=\arg\max\log p_{\theta}(\bar{\mathbf{x}})$
\end_inset

.
 This is because when the Oracle can forecast values of 
\begin_inset Formula $X$
\end_inset

 with certainty, it is as if we have already observed such values, and the
 maximum likelihood techniques described above can be applied.
 From this heuristic, we may consider restricting the space of possible
 candidates to those such that 
\begin_inset Formula $\Lambda^{\delta_{\bar{\mathbf{x}}}}(\theta)=\log p_{\theta}(\bar{\mathbf{x}})$
\end_inset

.
 An obvious possibility, then, could be 
\begin_inset Formula 
\begin{equation}
\Lambda^{\nu}(\theta)\coloneqq\int\log p_{\theta}(\mathbf{x})\nu(d\mathbf{x}).\label{eq:Lambda}
\end{equation}

\end_inset

Though this particular definition 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

 indeed satisfies the heuristic, there are many others that would as well.
 In the next subsection, we will explore the properties of 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

 and demonstrate why it is a natural quantity to maximize as an analog to
 maximum likelihood estimation in the context of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prob:problem-oracle"

\end_inset

.
\end_layout

\begin_layout Subsection
The Properties of 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset


\end_layout

\begin_layout Standard
To begin our exploration of 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

, we present an observation made in 
\begin_inset CommandInset citation
LatexCommand citet
key "akaike-1973"

\end_inset

, which will allows us to understand what it is that we are optimizing for
 when we maximize the quantity in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Lambda"

\end_inset

.
\end_layout

\begin_layout Proposition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "akaike-1973"

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "prop:akaike"

\end_inset

Consider a family of distributions 
\begin_inset Formula $\mathscr{F}=\{F_{\theta}\}_{\theta\in\Theta}$
\end_inset

 with Lebesgue densities 
\begin_inset Formula $f_{\theta}$
\end_inset

.
 Fix 
\begin_inset Formula $\theta_{0}\in\Theta$
\end_inset

, and let 
\begin_inset Formula $X$
\end_inset

 be a random variable distributed according to 
\begin_inset Formula $F_{\theta_{0}}$
\end_inset

.
 Then, 
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}\mathbb{E}_{f_{\theta_{0}}}[\log f_{\theta}(X)]=\arg\min_{\theta\in\Theta}\mathbb{E}_{f_{\theta_{0}}}\left[\log\frac{f_{\theta_{0}}(X)}{f_{\theta}(X)}\right].
\]

\end_inset


\end_layout

\begin_layout Standard
The proof of this result is straightforward and does not require that the
 distribution of 
\begin_inset Formula $X$
\end_inset

 be an element of 
\begin_inset Formula $\mathscr{F}$
\end_inset

.
 In particular, we only require that the distribution of 
\begin_inset Formula $X$
\end_inset

 is absolutely continuous with respect to any distribution in 
\begin_inset Formula $\mathscr{F}$
\end_inset

.
 Therefore, we can view maximizing 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

 (which we will refer to as maximum 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

 estimation, or M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E) as minimizing the Kullback-Liebler divergence of 
\begin_inset Formula $\mathbb{P}_{\mathcal{T}}(\cdot\mid\theta)$
\end_inset

 from 
\begin_inset Formula $\nu$
\end_inset

, or roughly speaking, minimizing the information lost when we use 
\begin_inset Formula $\mathbb{P}_{\mathcal{T}}(\cdot\mid\theta)$
\end_inset

 to approximate 
\begin_inset Formula $\nu$
\end_inset

 if 
\begin_inset Formula $\nu\not\in\{\mathbb{P}_{\mathcal{T}}\mid\mathbb{P}\in\mathscr{P}\}$
\end_inset

.
 This result is promising; it suggests that maximizing 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

 is in fact statistically meaningful.
 However, we still lack a concrete link between M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E and MLE, which should exist if we are to claim that M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E is a meaningful generalization of MLE for distributional data.
 The next result explicitly makes this connection.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:misspecification"

\end_inset

The maximum likelihood estimate on discretely observed data at times 
\begin_inset Formula $\mathcal{T}$
\end_inset

 under model misspecification converges almost surely to the maximum 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

 estimate as the sample size grows.
\end_layout

\begin_layout Proof
Consider 
\begin_inset Formula $x_{\mathcal{T}}^{(1)},\dots,x_{\mathcal{T}}^{(N)}$
\end_inset

, 
\begin_inset Formula $N$
\end_inset

 i.i.d.
 draws from 
\begin_inset Formula $\nu$
\end_inset

, where 
\begin_inset Formula $x_{\mathcal{T}}^{(i)}$
\end_inset

 is as before a 
\begin_inset Formula $1\times n$
\end_inset

 vector.
 Then, let 
\begin_inset Formula 
\[
\hat{\theta}_{N}=\arg\max_{\theta\in\Theta}\frac{1}{N}\sum_{i=1}^{N}\log p_{\theta}(x_{\mathcal{T}}^{(i)})
\]

\end_inset

be the quasi-maximum likelihood estimator.
 By a result of 
\begin_inset CommandInset citation
LatexCommand citet
before "Theorem 2.2 in"
key "white-1982"

\end_inset

, 
\begin_inset Formula 
\[
\hat{\theta}_{N}\xrightarrow{a.s.}\arg\min_{\theta\in\Theta}\mathbb{E}\left[\log\frac{v(X_{\mathcal{T}})}{p_{\theta}(X_{\mathcal{T}})}\right]
\]

\end_inset

as 
\begin_inset Formula $N\rightarrow\infty$
\end_inset

, and thus by the generalized statement of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:akaike"

\end_inset

, 
\begin_inset Formula 
\[
\hat{\theta}_{N}\xrightarrow{a.s.}\arg\max_{\theta\in\Theta}\Lambda^{\nu}(\theta).
\]

\end_inset

as desired.
\end_layout

\begin_layout Standard
The intuition for the relationship between model misspecification and M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E as specified in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:misspecification"

\end_inset

 is as follows.
 Recall that by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:markov-construction-thm"

\end_inset

, a Markov measure is uniquely determined by its transition density, which
 itself is uniquely determined by a particular SDE, and a marginal distribution.
 Restricting ourselves to the stationary solutions to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

, there is an bijection between 
\begin_inset Formula $\Theta$
\end_inset

 and 
\begin_inset Formula $\mathscr{P}$
\end_inset

.
 In particular, the measures in 
\begin_inset Formula $\mathscr{P}$
\end_inset

 are the only possible measures that any stationary solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 could induce.
 Therefore, if the Oracle tells us that in fact, 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 will be distributed according to 
\begin_inset Formula $\nu$
\end_inset

 (perhaps more intuitively, in the limit as the Oracle gives us an increasing
 number of draws from 
\begin_inset Formula $\nu$
\end_inset

), and 
\begin_inset Formula $\nu\not\in\{\mathbb{P}_{\mathcal{T}}\mid\mathbb{P}\in\mathscr{P}\}$
\end_inset

, we must have misspecified our model.
 In this case, the best estimate we can make for 
\begin_inset Formula $\theta$
\end_inset

 is to find the element in 
\begin_inset Formula $\mathscr{P}$
\end_inset

, which when restricted to 
\begin_inset Formula $\mathcal{T}$
\end_inset

, most closely approximates 
\begin_inset Formula $\nu$
\end_inset

 i.e.
 minimizes the Kullback-Liebler divergence of that element from 
\begin_inset Formula $\nu$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:misspecification"

\end_inset

 also allows us to formalize the sense in which maximum 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

 estimation is equivalent to maximum likelihood estimation when 
\begin_inset Formula $\nu$
\end_inset

 is a degenerate distribution.
\end_layout

\begin_layout Corollary
When 
\begin_inset Formula $\nu=\delta_{\bar{\mathbf{x}}}$
\end_inset

 for some fixed data 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

, the outcome of maximum likelihood estimation on 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

 is equal to the outcome of maximum 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

 estimation.
\end_layout

\begin_layout Proof
Simply note that the hypothesis reduces 
\begin_inset Formula $\hat{\theta}_{N}$
\end_inset

 in the proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:misspecification"

\end_inset

 to 
\begin_inset Formula $\hat{\theta}=\arg\max_{\theta\in\Theta}\log p_{\theta}(\bar{\mathbf{x}})$
\end_inset

 for all 
\begin_inset Formula $N$
\end_inset

.
 The corollary immediately follows.
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:misspecification"

\end_inset

 is powerful because it allows us to address a further generalized version
 of the problem of the Oracle.
 For we can view the current specification of the problem as one in which
 the Oracle knows the true distribution of 
\begin_inset Formula $X_{\mathcal{T}}$
\end_inset

 to be 
\begin_inset Formula $\nu$
\end_inset

, and gives us an infinite number of draws from 
\begin_inset Formula $\nu$
\end_inset

 to conduct inference on.
 However, we can suppose the Oracle chooses only to give us some finite
 number 
\begin_inset Formula $N$
\end_inset

 of samples from 
\begin_inset Formula $\nu$
\end_inset

.
 Then, the problem of the Oracle reduces to regular maximum likelihood estimatio
n under model misspecification, the theory of which has been developed in
 detail beginning with 
\begin_inset CommandInset citation
LatexCommand citet
key "white-1982"

\end_inset

.
 
\end_layout

\begin_layout Standard
At this point, it ought to be clear that 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

 as defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Lambda"

\end_inset

 is a reasonable, and perhaps the most reasonable (see 
\begin_inset CommandInset citation
LatexCommand citet
key "akaike-1973"

\end_inset

), choice of the infinitely many possible alternatives.
 As such, M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E is the appropriate generalization of MLE we have been searching for.
 However, given the unavailability of 
\begin_inset Formula $p_{t}(x,y;\theta)$
\end_inset

 for most diffusion processes, it quickly becomes clear that 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

 has no obvious analytical form for all but the most specific choices of
 
\begin_inset Formula $\mu_{\theta},\sigma_{\theta}$
\end_inset

, and 
\begin_inset Formula $\nu$
\end_inset

.
 Therefore, at this point, M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E is effectively impossible to carry out, and the next section will deveop
 a simulation-based strategy for conducting M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E.
\end_layout

\begin_layout Section
Inference on Almost-Prescient Soothsayings
\end_layout

\begin_layout Standard
We generalize the results of 
\begin_inset CommandInset citation
LatexCommand citet
key "roberts-stramer-2001,beskos-2006,bladt-sorensen-2014"

\end_inset

 to propose a simulation-based method to conduct M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E.
 As noted above, even writing out 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

, let alone maximizing it analytically, is impossible in general.
 However, noting the close parallels between MLE on discretely observed
 data and M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E on distributional data, we develop in this section a Monte Carlo simulation
 strategy which converges to the maximum 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

 estimate.
 We will also note that the 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 algorithm is a special case of the below algorithm when 
\begin_inset Formula $\nu=\delta_{\bar{\mathbf{x}}}$
\end_inset

 for some fixed data 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

.
\end_layout

\begin_layout Subsection
EM on Processes with Unit Diffusion Coefficient
\end_layout

\begin_layout Standard
We will first consider the highly specific case of 
\begin_inset Formula $\sigma(x;\theta)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 being equal to unity.
 Under this strong assumption, we will derive an EM algorithm that will
 maximize 
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

, in a manner analagous to the EM algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 which maximizes the log-likelihood of discretely observed data.
 We first prove a theorem that will allow us to derive such an algorithm,
 recalling that the likelihood of a continuous path of a solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 where 
\begin_inset Formula $\sigma(x;\theta)=1$
\end_inset

 is readily available by Girsanov's theorem.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:em"

\end_inset

Fix an parameter estimate 
\begin_inset Formula $\tilde{\theta}\in\Theta$
\end_inset

.
 Then, 
\begin_inset Formula 
\begin{equation}
\arg\max_{\theta\in\Theta}\Lambda^{\nu}(\theta)-\Lambda^{\nu}(\tilde{\theta})=\arg\max_{\theta\in\Theta}\mathbb{E}_{\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})}\left[\log p_{\theta}(X)\right],\label{eq:em-equation}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})$
\end_inset

 is the Baudoin 
\begin_inset Formula $(X_{\mathcal{T}},\nu)$
\end_inset

-conditioning of the measure induced by a stationary solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 with parameter 
\begin_inset Formula $\tilde{\theta}$
\end_inset

, and 
\begin_inset Formula $X$
\end_inset

 is a random path distributed according to 
\begin_inset Formula $\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})$
\end_inset

.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $X$
\end_inset

 be a stationary solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 as usual.
 Recall that by assumption, 
\begin_inset Formula $\mathbf{X}\coloneqq X_{\mathcal{T}}$
\end_inset

 is distributed according to 
\begin_inset Formula $\nu$
\end_inset

.
 We begin by re-expressing the objective function on the left hand side
 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:em-equation"

\end_inset

 as 
\begin_inset Formula 
\begin{eqnarray}
\Lambda^{\nu}(\theta)-\Lambda^{\nu}(\tilde{\theta}) & = & \int\log p_{\theta}(\mathbf{x})\nu(d\mathbf{x})-\Lambda^{\nu}(\tilde{\theta})\nonumber \\
 & = & \int\log\int p_{\theta}(\mathbf{x},z)dz\nu(d\mathbf{x})-\Lambda^{\nu}(\tilde{\theta}),\label{eq:firststepem}
\end{eqnarray}

\end_inset

where we take
\series bold
 
\begin_inset Formula $Z$
\end_inset

 
\series default
to be missing data representing the path of 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $t\in[0,1]$
\end_inset

.
 Then, following standard arguments in the derivation of EM algorithms (see
 
\begin_inset CommandInset citation
LatexCommand citet
key "borman-2006"

\end_inset

), we can find from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:firststepem"

\end_inset

 that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align}
\Lambda^{\nu}(\theta)-\Lambda^{\nu}(\tilde{\theta}) & \geq\iint\log\left(\frac{p_{\theta}(\mathbf{x},z)}{p_{\tilde{\theta}}(z\mid\mathbf{x})}\right)p_{\tilde{\theta}}(z\mid\mathbf{x})dz\nu(d\mathbf{x})-\Lambda^{\nu}(\tilde{\theta})\nonumber \\
 & =\iint\log\left(\frac{p_{\theta}(\mathbf{x},z)}{p_{\tilde{\theta}}(z\mid\mathbf{x})p_{\tilde{\theta}}(\mathbf{x})}\right)p_{\tilde{\theta}}(z\mid\mathbf{x})dz\nu(d\mathbf{x}).\label{eq:Deltatn}
\end{align}

\end_inset


\end_layout

\begin_layout Proof
Assuming suitable regularity conditions on the densities, and recalling
 that by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:pv-density-1"

\end_inset

, a Baudoin conditioning is absolutely continuous with respect to its base
 measure, note that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Deltatn"

\end_inset

 can be re-written as 
\begin_inset Formula 
\[
\Delta(\theta\mid\tilde{\theta})\coloneqq\int\log\left(\frac{p_{\theta}(\mathbf{x},z)}{p_{\tilde{\theta}}(z\mid\mathbf{x})p_{\tilde{\theta}}(\mathbf{x})}\right)d\mathbb{P}^{\nu}(z\mid\tilde{\theta})
\]

\end_inset

by the definition of a Baudoin conditioning.
 Now, let 
\begin_inset Formula $Q(\theta\mid\tilde{\theta})\coloneqq\Lambda^{\nu}(\tilde{\theta})+\Delta(\theta\mid\tilde{\theta})\leq\Lambda^{\nu}(\theta)$
\end_inset

.
 By standard arguments, any 
\begin_inset Formula $\theta$
\end_inset

 that increases 
\begin_inset Formula $Q(\theta\mid\tilde{\theta})$
\end_inset

 must also increase 
\begin_inset Formula $\Lambda^{\nu}(\theta)$
\end_inset

.
 To maximize this increase, it suffices to solve for
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}Q(\theta\mid\tilde{\theta})=\arg\max_{\theta\in\Theta}\mathbb{E}_{\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})}\left[\log p_{\theta}(X)\right],
\]

\end_inset

and the theorem follows as desired.
\end_layout

\begin_layout Standard
Note that when 
\begin_inset Formula $\nu=\delta_{\bar{\mathbf{x}}}$
\end_inset

 for fixed data 
\begin_inset Formula $\bar{\mathbf{x}}$
\end_inset

, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:em-equation"

\end_inset

 reduces to 
\begin_inset Formula 
\[
\arg\max_{\theta\in\Theta}\ell(\theta\mid\bar{\mathbf{x}})-\ell(\tilde{\theta}\mid\bar{\mathbf{x}})=\arg\max_{\theta}\mathbb{E}_{Z\mid\bar{\mathbf{x}},\tilde{\theta}}\left[\log f(Z,\bar{\mathbf{x}}\mid\theta)\right]
\]

\end_inset

where the expectation is taken over diffusion bridges 
\begin_inset Formula $Z$
\end_inset

, as presented in 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 for the case of discretely observed data.
 
\end_layout

\begin_layout Standard
The only task that remains is to find a functional form for 
\begin_inset Formula $\log p_{\theta}(x)$
\end_inset

 when 
\begin_inset Formula $x$
\end_inset

 is a complete diffusion path.
 In general, by Girsanov's theorem, we know that 
\begin_inset Formula 
\[
\log p_{\theta}(x)=\int_{0}^{1}\mu_{\theta}(x_{s})dx_{s}-\frac{1}{2}\int_{0}^{1}\mu_{\theta}^{2}(x_{s})ds
\]

\end_inset

and thus the expectation step of an EM algorithm would consist of computing
 
\begin_inset Formula 
\begin{eqnarray}
Q(\theta\mid\tilde{\theta}) & = & \mathbb{E}_{\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})}\left[\int_{0}^{1}\mu_{\theta}(X_{s})dX_{s}-\frac{1}{2}\int_{0}^{1}\mu_{\theta}^{2}(X_{s})ds\right]\nonumber \\
 & = & \mathbb{E}_{\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})\times\mathbb{U}}\left[A_{\theta}(X_{1})-A_{\theta}(X_{0})-\frac{1}{2}\left(\mu_{\theta}^{2}+\mu_{\theta}^{'}\right)(X_{U})\right]\label{eq:q-simple}
\end{eqnarray}

\end_inset

for independent 
\begin_inset Formula $U\sim\mbox{Unif}$
\end_inset

.
 Arguments that 
\begin_inset Formula $Q(\theta\mid\tilde{\theta})$
\end_inset

 is the relevant likelihood function can be found in 
\begin_inset CommandInset citation
LatexCommand citet
key "papa-roberts-2012"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

; though still intractable analytically, its form hints at a way we may
 adopt a Monte Carlo implementation of the EM algorithm suggested by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:em"

\end_inset

 following 
\begin_inset CommandInset citation
LatexCommand citet
key "wei-tanner-1990"

\end_inset

 if we could simulate from 
\begin_inset Formula $\mathbb{P}^{\nu}(\cdot\mid\tilde{\theta})$
\end_inset

.
 We give a detailed outline of such an implementation in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "algo:mcem"

\end_inset

, after developing theory to handle diffusions with non-unit diffusion coefficie
nt in the next subsection.
\end_layout

\begin_layout Standard
We would also like to point out that in the case of only having access to
 a finite number of samples from the true distribution 
\begin_inset Formula $\nu$
\end_inset

, instead of the distribution 
\begin_inset Formula $\nu$
\end_inset

 itself, the derivation of the above algorithm goes through mutatis mutandis,
 using the empirical distribution of the samples as a stand-in for 
\begin_inset Formula $\nu$
\end_inset

.
 In this case, the EM algorithm will converge to the maximum likelihood
 estimate under model misspecification as outlined in the previous section.
 This estimate is still consistent for the Kullback-Liebler divergence minimizin
g parameter value, and 
\begin_inset CommandInset citation
LatexCommand citet
key "white-1982"

\end_inset

 gives asymptotics for the efficiency of such an estimator.
\end_layout

\begin_layout Subsection
Augmentation Under the Lamperti Transform
\end_layout

\begin_layout Standard
The derivation of the algorithm above assumed a unit diffusion coefficient
 for ease of exposition.
 In particular, we were able to use Girsanov's theorem to find the complete
 log-likelihood of a path of 
\begin_inset Formula $X$
\end_inset

, which allowed for the derivation of an EM algorithm suggested by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:em"

\end_inset

.
 However, the unit diffusion coefficient requirement is far too stringent
 for the EM algorithm to be used in practice.
 As such, in this subsection, we relax this assumption and derive a similar
 EM algorithm for general use.
\end_layout

\begin_layout Standard
The problem of non-unit diffusion is easily addressed by the Lamperti transform
 
\begin_inset Formula $\eta_{\theta}(x)$
\end_inset

, where 
\begin_inset Formula 
\begin{equation}
\eta_{\theta}(x)=\int_{x^{*}}^{x}\frac{1}{\sigma_{\theta}(y)}dy`\label{eq:lamperti}
\end{equation}

\end_inset

for appropriate but otherwise arbitrary 
\begin_inset Formula $x^{*}$
\end_inset

.
 If we define 
\begin_inset Formula $Y_{t}(\theta)=\eta_{\theta}(X_{t})$
\end_inset

, by Ito's lemma, 
\begin_inset Formula $Y$
\end_inset

 is the solution to the stochastic differential equation
\begin_inset Formula 
\begin{equation}
dY_{t}=\alpha_{\theta}(Y_{t})dt+dW_{t},\label{eq:eta-y}
\end{equation}

\end_inset

where 
\begin_inset Formula 
\[
\alpha_{\theta}(y)=\frac{\mu_{\theta}(\eta_{\theta}^{-1}(y))}{\sigma_{\theta}(\eta_{\theta}^{-1}(y))}-\frac{1}{2}\sigma_{\theta}^{'}(\eta_{\theta}^{-1}(y)).
\]

\end_inset

While 
\begin_inset Formula $Y$
\end_inset

 is now a diffusion with unit coefficient and the theory developed in the
 previous subsection applies, we now face a new problem.
 Note that 
\begin_inset Formula $Y$
\end_inset

 is a function of 
\begin_inset Formula $\theta$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "beskos-2006"

\end_inset

 point out that a continuous-time diffusion path on 
\begin_inset Formula $[0,t=1]$
\end_inset

 can be used to perfectly estimate the parameters of the diffusion coefficient,
 while perfect estimation from such data for the parameters of the drift
 coefficient can only come asymptotically as 
\begin_inset Formula $t\rightarrow\infty$
\end_inset

.
 As such, the implication is that since the missing data under the Lamperti
 transform is a function of 
\begin_inset Formula $\theta$
\end_inset

, augmenting our EM algorithm with the missing path of 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $[0,1]$
\end_inset

 would introduce an unacceptably high fraction of missing information (in
 particular, the fraction of missing information would be equal to 
\begin_inset Formula $1$
\end_inset

) which would prevent EM from converging.
\end_layout

\begin_layout Standard
As such, we must augment our EM scheme with missing data from which we could
 not perfectly estimate 
\begin_inset Formula $\theta$
\end_inset

 in a finite amount of time.
 We generalize the method of 
\begin_inset CommandInset citation
LatexCommand citet
key "bladt-sorensen-2014"

\end_inset

 following 
\begin_inset CommandInset citation
LatexCommand citet
key "roberts-stramer-2001"

\end_inset

 and consider the Baudoin 
\begin_inset Formula $(Y_{\mathcal{T}},\nu\circ\eta_{\theta}^{-1})$
\end_inset

-bridge of a solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:eta-y"

\end_inset

 with parameter 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 We will call this Baudoin bridge 
\begin_inset Formula $Y^{*}(\theta,\tilde{\theta})$
\end_inset

.
 Note that we could not perfectly estimate 
\begin_inset Formula $\theta$
\end_inset

 using only the missing data for 
\begin_inset Formula $Y^{*}(\theta,\tilde{\theta})$
\end_inset

 i.e.
 the path of this bridge at times other than 
\begin_inset Formula $\mathcal{T}$
\end_inset

.
  Furthermore, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:pv-density-1"

\end_inset

, the measure induced by 
\begin_inset Formula $Y^{*}$
\end_inset

 is absolutely continuous with respect to the Wiener measure, and therefore
 admits a complete log-likelihood by Girsanov's theorem.
 
\end_layout

\begin_layout Standard
The likelihood of a path 
\begin_inset Formula $y$
\end_inset

 of 
\begin_inset Formula $\eta_{\theta}(X)$
\end_inset

 is
\begin_inset Formula 
\[
\mathcal{N}(x_{1}-x_{0})\exp\left\{ A_{\theta}(x_{1})-A_{\theta}(x_{0})-\frac{1}{2}\int\left(\alpha_{\theta}^{2}+\alpha_{\theta}^{'}\right)\left(x_{s}\right)ds\right\} \left|\eta_{\theta}^{'}(w)\right|
\]

\end_inset


\end_layout

\begin_layout Proposition
The log-likelihood of a complete path 
\begin_inset Formula $\omega$
\end_inset

 of 
\begin_inset Formula $Y^{*}(\theta,\tilde{\theta})$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
f_{\theta,\tilde{\theta}}(\omega)=\mathbb{E}_{\nu\circ\eta_{\theta}^{-1}}\left[A_{\tilde{\theta}}(\omega_{1})-A_{\tilde{\theta}}(\omega_{0})\right]-\frac{1}{2}\int_{0}^{1}(\alpha_{\tilde{\theta}}^{2}+\alpha_{\tilde{\theta}}^{'})(\omega_{s})ds,\label{eq:complete-y-star}
\end{equation}

\end_inset

where the expectation is taken with respect to the values of 
\begin_inset Formula $\omega_{\mathcal{T}}$
\end_inset

, which is distributed according to law 
\begin_inset Formula $\nu\circ\eta_{\theta}^{-1}$
\end_inset

.
\end_layout

\begin_layout Proof
By the definition of a Baudoin conditioning, we know the measure 
\begin_inset Formula $\mathbb{Y}^{*}(\cdot\mid\theta,\tilde{\theta})$
\end_inset

 induced by 
\begin_inset Formula $Y^{*}(\theta,\tilde{\theta})$
\end_inset

 can be disintegrated as 
\begin_inset Formula 
\[
\mathbb{Y}^{*}(\cdot\mid\theta,\tilde{\theta})=\int\mathbb{Y}(\cdot\mid\tilde{\theta},Y_{\mathcal{T}}=y_{\mathcal{T}})(\nu\circ\eta_{\theta}^{-1})(dy_{\mathcal{T}}),
\]

\end_inset

where 
\begin_inset Formula $\mathbb{Y}$
\end_inset

 is the measure induced by 
\begin_inset Formula $Y$
\end_inset

.
 
\begin_inset Marginal
status open

\begin_layout Plain Layout
I am very unsure about this proof, since the other papers use 
\begin_inset Formula $\theta$
\end_inset

 instead of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 as the parameter for the 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

 functions.
\end_layout

\end_inset

Assuming the requisite Lebesgue densities exist and are sufficiently regular,
 it suffices to find the Lebesgue density of 
\begin_inset Formula $\mathbb{Y}(\cdot\mid\tilde{\theta},Y_{\mathcal{T}}=y_{\mathcal{T}})$
\end_inset

.
 By a result of 
\begin_inset CommandInset citation
LatexCommand citet
before "Lemma 1 in"
key "beskos-2006"

\end_inset

, we know the log-density of 
\begin_inset Formula $\mathbb{Y}(\cdot\mid\tilde{\theta},Y_{\mathcal{T}}=y_{\mathcal{T}})$
\end_inset

 is 
\begin_inset Formula 
\[
f_{\tilde{\theta}}(\omega\mid\omega_{\mathcal{T}})=A_{\tilde{\theta}}(\omega_{1})-A_{\tilde{\theta}}(\omega_{0})-\frac{1}{2}\int_{0}^{1}(\alpha_{\tilde{\theta}}^{2}+\alpha_{\tilde{\theta}}^{'})(\omega_{s})ds,
\]

\end_inset

where 
\begin_inset Formula $A_{\theta}(x)=\int_{x^{*}}^{x}\alpha_{\theta}(z)dz$
\end_inset

 for arbitrary but appropriate 
\begin_inset Formula $x^{*}$
\end_inset

, for some sample path 
\begin_inset Formula $\omega$
\end_inset

.
 The result follows as desired.
 
\end_layout

\begin_layout Standard
With the complete log-likelihood of a path of 
\begin_inset Formula $Y^{*}(\theta,\tilde{\theta})$
\end_inset

, we are ready to derive a generalized algorithm for performing M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E in the final subsection of this chapter.
\end_layout

\begin_layout Subsection
A Generalized MCEM Algorithm
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MCEM for M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E on Distributional Data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
Function{$Q$}{$
\backslash
theta, 
\backslash
tilde{
\backslash
theta}, N$}
\end_layout

\begin_layout Plain Layout

	
\backslash
For{$i = 1,
\backslash
dots, N$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State{$Y_
\backslash
mathcal{T}^{(i)} 
\backslash
gets 
\backslash
eta_{
\backslash
theta}(X_
\backslash
mathcal{T}^{(i)})$, where $X_
\backslash
mathcal{T}^{(i)} 
\backslash
sim 
\backslash
nu$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State{$Y^{*,(i)} 
\backslash
gets$ 
\backslash
Call{exact $
\backslash
nu$-bridge}{$
\backslash
mu_{
\backslash
tilde{
\backslash
theta}}, 
\backslash
sigma_{
\backslash
tilde{
\backslash
theta}}, 
\backslash
nu 
\backslash
circ 
\backslash
eta_
\backslash
theta^{-1}, 
\backslash
mathbf{T}$}}
\end_layout

\begin_layout Plain Layout

		
\backslash
State{$U^{(i)} 
\backslash
gets$ 
\backslash
Call{sample}{$
\backslash
mathbf{T}$}}
\end_layout

\begin_layout Plain Layout

	
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return{${
\backslash
displaystyle
\backslash
frac{1}{N} 
\backslash
sum^{N}_{i=1}  [
\backslash
alpha_{
\backslash
tilde{
\backslash
theta}}(Y_1^{(i)}) - 
\backslash
alpha_{
\backslash
tilde{
\backslash
theta}}(Y_0^{(i)})] - 
\end_layout

\begin_layout Plain Layout

					 			 
\backslash
frac{1}{2N}
\backslash
sum^{N}_{i=1} (
\backslash
mu_{
\backslash
tilde{
\backslash
theta}}^2 + 
\backslash
mu_{
\backslash
tilde{
\backslash
theta}}^{'})(Y^{*,(i)}_{U^{(i)}})}$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{M$
\backslash
Lambda^
\backslash
nu$E}{$
\backslash
theta_0, N$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$
\backslash
tilde{
\backslash
theta} 
\backslash
gets 
\backslash
theta_0$}	
\end_layout

\begin_layout Plain Layout

	
\backslash
State{$
\backslash
mathbf{repeat
\backslash
;} 
\backslash
tilde{
\backslash
theta} = 
\backslash
arg 
\backslash
max_{
\backslash
theta 
\backslash
in 
\backslash
Theta} Q(
\backslash
theta, 
\backslash
tilde{
\backslash
theta}, N)
\backslash
;$$
\backslash
mathbf{until}$ convergence}
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return{$
\backslash
tilde{
\backslash
theta}$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "algo:mcem"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Using the complete log-likelihood in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:complete-y-star"

\end_inset

, we may write down the general 
\begin_inset Formula $Q$
\end_inset

 function to optimize in our EM algorithm to conduct inference on a general
 solution to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde-with-theta"

\end_inset

 given distributional data 
\begin_inset Formula $\nu$
\end_inset

.
 Following 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:q-simple"

\end_inset

, we write 
\begin_inset Formula 
\begin{eqnarray}
Q(\theta\mid\tilde{\theta}) & = & \mathbb{E}_{\nu\circ\eta_{\theta}^{-1}}\left[A_{\tilde{\theta}}(Y_{1})-A_{\tilde{\theta}}(Y_{0})\right]-\nonumber \\
 &  & \frac{1}{2}\mathbb{E}_{\mathbb{Y}^{\nu\circ\eta_{\theta}^{-1}}(\cdot\mid\tilde{\theta})\times\mathbb{U}}\left[(\alpha_{\tilde{\theta}}^{2}+\alpha_{\tilde{\theta}}^{'})(Y_{U}^{*}(\theta,\tilde{\theta}))\right]\label{eq:general-q}
\end{eqnarray}

\end_inset

Since these expectations are not available analytically, we follow 
\begin_inset CommandInset citation
LatexCommand citet
key "wei-tanner-1990"

\end_inset

 and implement a Monte Carlo EM algorithm, presented in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "algo:mcem"

\end_inset

.
 This algorithm assumes the existence of 
\noun on
exact 
\begin_inset Formula $\nu$
\end_inset

-bridge
\noun default
(
\series bold
\noun on

\begin_inset Formula $\mu_{\theta},\sigma_{\theta},\nu,\mbox{\textbf{T}}$
\end_inset


\series default
\noun default
), a method that samples processes exactly from 
\begin_inset Formula $\mathbb{P}^{\nu}(\cdot\mid\theta)$
\end_inset

 and returns discrete observations of such processes at times 
\begin_inset Formula $\mathbf{T}=\{0,\delta,2\delta\dots,M\delta\}$
\end_inset

 for 
\begin_inset Formula $\delta=1/M$
\end_inset

.
\end_layout

\begin_layout Standard
While having this algorithm in hand is exciting, there remains a large elephant
 in the room.
 The theory developed throughout this chapter is interesting, but effectively
 useless without a way to actually sample from 
\begin_inset Formula $\mathbb{P}^{\nu}(\cdot\mid\theta)$
\end_inset

.
 Fortunately for this thesis, we are able to derive such a sampler in the
 next chapter, which can be plugged into 
\begin_inset CommandInset ref
LatexCommand formatted
reference "algo:mcem"

\end_inset

 to actually conduct M
\begin_inset Formula $\Lambda^{\nu}$
\end_inset

E in practice.
\end_layout

\end_body
\end_document
